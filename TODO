- use malloc/free when moving blocks to/from GPU. This way the GPU node does not need as much memory (but we can use swap...)
- write kernels in .h files that can be included by both openmp and cuda code.
- try using local memory for derivatives in CUDA (avoid jumping in memory)
- CUDA performance of Crank-Nicolson is poor... too complex kernels. There are many if's but at least they don't diverge.
- Poisson solver for other than periodic BC
- Fortran support: iso_c_binding, interface / end interface.
- CUDA: most things have periodic BCs hardwired...
- For mem2gpu we need to track in which space we are so that we can upload the data to GPU in the correct subformat. 
- GPU shuffle routines not tested.
- FFT abs boundaries for predict-correct.
- update manual - it is now quite out of date.
- Implement cfunction the same way as rfunction. These functions need to use linear interpolation...

also fgrep TODO *.[ch] to find out more specific issues.

=====================================

CUDA performance next:

1. Chcek if we have pointer aliasing issues (__restrict__)

2. We are creating too many small threads (kernel launch overhead and integer arithmetic for index calculations).
A quick test shows that at least simple kernels can be sped up by over 3X by scheduling the last index of the
grid to the threads. Here is a sample code:

/*
 * Grid abs power device code.
 *
 * dst = POW(|src|,x)
 *
 */

__global__ void cgrid_cuda_abs_power_gpu(CUCOMPLEX *dst, CUCOMPLEX *src, CUREAL x, INT nx, INT ny, INT nz) {

  INT j = blockIdx.y * blockDim.y + threadIdx.y, i = blockIdx.z * blockDim.z + threadIdx.z, idx, tmp;

  if(i >= nx || j >= ny) return;

  tmp = (i * ny + j) * nz;

  for (idx = tmp; idx < tmp + nz; idx++) {
    dst[idx].x = POW(CUCREAL(src[idx]) * CUCREAL(src[idx]) + CUCIMAG(src[idx]) * CUCIMAG(src[idx]), x / 2.0);
    dst[idx].y = 0.0;
  }
}

/*
 * Grid abs power.
 *
 * dst      = Destination for operation (gpu_mem_block *; input).
 * src      = Source for operation (gpu_mem_block *; input).
 * nx       = # of points along x (INT; input).
 * ny       = # of points along y (INT; input).
 * nz       = # of points along z (INT; input).
 *
 * In real space.
 *
 */

extern "C" void cgrid_cuda_abs_powerW(gpu_mem_block *dst, gpu_mem_block *src, CUREAL exponent, INT nx, INT ny, INT nz) {

  dst->gpu_info->subFormat = src->gpu_info->subFormat;
  INT i, ngpu2 = dst->gpu_info->descriptor->nGPUs, ngpu1, nnx1, nnx2, nny1, nny2;
                        dim3 threads(CUDA_THREADS_PER_BLOCK, CUDA_THREADS_PER_BLOCK, 1); 
                        dim3 blocks1, blocks2; 
                        if(dst->gpu_info->subFormat == CUFFT_XT_FORMAT_INPLACE) { 
                          ngpu1 = nx % ngpu2; 
                          nnx2 = nx / ngpu2; 
                          nnx1 = nnx2 + 1; 
                          nny1 = ny;
                          nny2 = ny;
                          blocks2.x = blocks1.x = (ny + CUDA_THREADS_PER_BLOCK - 1) / CUDA_THREADS_PER_BLOCK; 
                          blocks1.y = (nnx1 + CUDA_THREADS_PER_BLOCK - 1) / CUDA_THREADS_PER_BLOCK; 
                          blocks2.y = (nnx2 + CUDA_THREADS_PER_BLOCK - 1) / CUDA_THREADS_PER_BLOCK; 
                          blocks1.z = blocks2.z = 1;
                        } else { 
                          ngpu1 = ny % ngpu2;
                          nny2 = ny / ngpu2;
                          nny1 = nny2 + 1;
                          nnx1 = nx;
                          nnx2 = nx;
                          blocks1.x = (nny1 + CUDA_THREADS_PER_BLOCK - 1) / CUDA_THREADS_PER_BLOCK; 
                          blocks2.x = (nny2 + CUDA_THREADS_PER_BLOCK - 1) / CUDA_THREADS_PER_BLOCK; 
                          blocks1.y = blocks2.y = (nx + CUDA_THREADS_PER_BLOCK - 1) / CUDA_THREADS_PER_BLOCK; 
                          blocks1.z = blocks2.z = 1;
                        }
  cudaXtDesc *SRC = src->gpu_info->descriptor, *DST = dst->gpu_info->descriptor;

  for(i = 0; i < ngpu1; i++) {
    cudaSetDevice(DST->GPUs[i]);
    cgrid_cuda_abs_power_gpu<<<blocks1,threads>>>((CUCOMPLEX *) DST->data[i], (CUCOMPLEX *) SRC->data[i], exponent, nnx1, nny1, nz);
  }

  for(i = ngpu1; i < ngpu2; i++) {
    cudaSetDevice(DST->GPUs[i]);
    cgrid_cuda_abs_power_gpu<<<blocks2,threads>>>((CUCOMPLEX *) DST->data[i], (CUCOMPLEX *) SRC->data[i], exponent, nnx2, nny2, nz);
  }

  cuda_error_check();
}

The thread and block structures are normally initialized by cuda-vars.h macros, so those need to be changed to conform with the above.
Note that this will require using X (or Y) in libdft 1D code rather than NZ in order the get more than one thread! 

Here is the simple benchmark code:

/*
 * Test benchmark.
 *
 */

#include <stdio.h>
#include <math.h>
#include <grid/grid.h>

/* Grid dimensions */
#define NX 1024
#define NY 512
#define NZ 512
#define STEP 0.2

/* If using CUDA, use the following GPU allocation */
#ifdef USE_CUDA
#define NGPUS 1
int gpus[NGPUS] = {0};
#endif

/* Right hand side function for Poisson equation (Gaussian) */
REAL complex gaussian(void *arg, REAL x, REAL y, REAL z) {

  REAL inv_width = 0.2;
  REAL norm = 0.5 * M_2_SQRTPI * inv_width;

  norm = norm * norm * norm;
  return norm * exp(-(x * x + y * y + z * z) * inv_width * inv_width);
}

int main(int argc, char **argv) {
  
  cgrid *grid;
  grid_timer timer;
  INT i;
  
  /* Initialize with all OpenMP threads */
  grid_threads_init(0);

  /* If libgrid was compiled with CUDA support, enable CUDA */
#ifdef USE_CUDA
  cuda_enable(1, NGPUS, gpus);
#endif
  
  /* Allocate real grid for the right hand side (and the solution) */
  grid = cgrid_alloc(NX, NY, NZ, STEP, CGRID_PERIODIC_BOUNDARY, NULL, "test");

  /* Map the right hand side to the grid */
  cgrid_map(grid, gaussian, NULL);

// Make sure we reside on GPU
  cgrid_fft(grid);
  cgrid_inverse_fft_norm(grid);

  grid_timer_start(&timer);
  for(i = 0; i < 10000; i++)
    cgrid_abs_power(grid, grid, 2.0);
  printf("Wall clock time = " FMT_R " seconds.\n", grid_timer_wall_clock_time(&timer));

  return 0;
}


